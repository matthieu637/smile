\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc} %fac
%\usepackage[utf8]{inputenc} %fac

%\usepackage[square,sort,comma]{natbib}
%\usepackage{chapterbib}

\usepackage[english]{babel} %FR
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{caption} %multiline caption

\usepackage[pdftex]{graphicx} % img
%\usepackage{wrapfig} %fac
\usepackage{float}

%\usepackage{algpseudocode} %fac/


\usepackage[top=3cm, bottom=3cm, left=2.5cm, right=2.5cm]{geometry} %Réduire les marges

% \usepackage{showkeys}
% \usepackage{showlabels}
\usepackage{nameref}

\usepackage{amsmath}
% \usepackage{algorithm2e}
\usepackage{algpseudocode}


% Style Page
\pagestyle{fancy} % entêtes

\setlength{\headheight}{15pt}
\lhead{ \leftmark }
\rhead{ %\rightmark 
}

\usepackage[style=authoryear,backend=bibtex,sortlocale=fr_FR]{biblatex}
\addbibresource{dependance/biblio.bib}


\newrobustcmd*{\parentexttrack}[1]{%
  \begingroup
  \blx@blxinit
  \blx@setsfcodes
  \blx@bibopenparen#1\blx@bibcloseparen
  \endgroup}

\AtEveryCite{%
  \let\parentext=\parentexttrack%
  \let\bibopenparen=\bibopenbracket%
  \let\bibcloseparen=\bibclosebracket}

\renewbibmacro*{cite}{%
  \iffieldundef{shorthand}
    {\ifthenelse{\ifnameundef{labelname}\OR\iffieldundef{labelyear}}
       {\usebibmacro{cite:label}%
        \setunit{\addspace}}
       {\printnames{labelname}%
        \setunit{\nameyeardelim}}%
%     \usebibmacro{cite:labelyear+extrayear}}% DELETED
     \printtext{(\usebibmacro{cite:labelyear+extrayear})}}% ADDED
    {\usebibmacro{cite:shorthand}}}

    
    


\sloppy % ne pas faire déborder les lignes dans la marge

\usepackage[hidelinks]{hyperref} %fac

%\setcounter{tocdepth}{1} %hideallsubsections

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\graphicspath{{images/}}

\begin{document}
  \begin{titlepage}
  \def\titletype{Experiments}
   \def\majortitle{Learning from expert}
   \def\docversion{1.1}
   \input{dependance/title_page.tex}
  \end{titlepage}

  
  \clearpage

  \tableofcontents
%   \addcontentsline{toc}{section}{Table de matière}
  
  %\vfill
  %\begin{center}
  %  \includegraphics[height=90mm]{images/torcs.png} 
  %\end{center}

  \clearpage
  
  \renewcommand{\labelitemi}{$\bullet$}
  \renewcommand{\labelitemii}{$\circ$}
  \renewcommand{\labelitemiii}{$\diamond$}
  \renewcommand{\labelitemiv}{$\ast$}
  
%   \section{Cadre : Apprentissage non supervisé par apprentissage des densités}

  \clearpage
  \section{Environnement}
    \subsection{Mountain car}

    A standard testing domain in reinforcement learning, is a problem in which an under-powered 
    car must drive up a steep hill. Since gravity is stronger than the car's engine, even at full 
    throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley
    and must learn to leverage potential energy by driving up the opposite hill before the car is 
    able to make it to the goal at the top of the rightmost hill, \cite{ReinforceLearningIntro} .
    \begin{figure}[H]
      \begin{center}
	\includegraphics[width=270px]{Mcar}
	\caption{ The Mountain Car Problem }
	\end{center}
    \end{figure}
    
    \paragraph{State Variables}
    Two dimensional continuous state space : \\
    $(x,y) \in Position \times Velocity $
      $ with \left\lbrace \begin{array}{lll} 
      Position = [-1.2, 0.6]\\
      Velocity = [-0.07, 0.07]\
      \end{array} \right.$
    
    
    
    \paragraph{Actions}
     One dimensional discrete action space : \\
     a $\in$ Motor = \{-1,0,+1\} corresponding respectivly to left, neutral, right
     
     \paragraph{Reward}
      For every time step: -1, when goal is reached: 0

    \paragraph{Transition}
     $ \left\lbrace \begin{array}{lll} 
      y = y + a \times 0.001+cos(3 \times x) \times -0.0025\\
      x = x + y
      \end{array} \right.$

    \paragraph{Initial State}
         $ \left\lbrace \begin{array}{lll} 
      x = -0.5\\
      y = 0.0 
      \end{array} \right.$
    
    \paragraph{Termination Condition}
    $x >= 0.6$
    
    
    \newpage
    \subsection{Open the Gate}

The mountain car problem gives a -1 reward at every step, and the state allows to 
    determine exactly the best action to take. So we could think that there is no state more important than other (to give feedbacks). 

Here, we introduce an other problem where rewards will be different according to our state.
    
    The agent have to reach every green boxes \textbf{according to the given order} to solve the problem.
    

    \begin{figure}[H]
      \begin{center}
	\includegraphics[width=270px]{gridworld}
	\caption{ Grid World }
	\end{center}
    \end{figure}
    
    \paragraph{State Variables}
    Three dimensional discretized state space :\newline
    $(x,y,n) \in \{1, 10\} \times \{1, 10\} \times \{1, 5\}$ \newline
    x and y are the coordinates of the grid\newline
    n determines which future case must be reached
    
    \paragraph{Actions}
    One dimensional discrete action space : the direction \newline
    $d \in \{left, up, right, bottom\}$ 

     \paragraph{Reward} 
      white boxes : -1 \newline
      green boxes : +20 if n = number\_box(x, y) 
                    else -1 
      \newline
      red boxes : -20 
      
      \paragraph{Transition}  
	x = x + 1 if d = right $\wedge \neg$ black\_box(x + 1, y) \newline
	y = y + 1 if d = up $\wedge \neg$ black\_box(x, y + 1) \newline
	x = x - 1 if d = left $\wedge \neg$ black\_box(x - 1, y) \newline
	y = y - 1 if d = bottom $\wedge \neg$ black\_box(x, y - 1) \newline
	n = n + 1 if green\_box(x, y) $\wedge$ n = number\_box(x, y) 
      
      \paragraph{Initial State}
      (x,y,n) = (1, 1 ,1)
      
      \paragraph{Termination Condition}
      n > 5
      \\[1.5cm]
      \paragraph{Conclusion}
      Thus, we have 2 environnement with different property :
      \begin{itemize}
       \item In each state, (except the goal) mountain car receives the same reward
       \item In each state, mountain car receives a negative reward (the Q function decreased over time)
       \item The grid environnement reverses these 2 points
      \end{itemize}

    
%     \subsection{Second Grid Game (LS)}
% 
%     The main difference in this problem, is that, given a state there isn't only one best possible action.
%      
%     The agent have to reach every green boxes in any order.
% 
%     \begin{figure}[H]
%       \begin{center}
% 	\includegraphics[width=270px]{gridworldls}
% 	\caption{ Gris World Insconsistent State }
% 	\end{center}
%     \end{figure}
%     
%     \subsubsection{State Variables}
%     Four dimensional discretized state space : X = \{0, 9\} ; Y = \{0, 9\}
%     
%     \subsubsection{Actions}
%      One dimensional discrete action space : Direction = (left, up, right, bottom)
%      
%      \subsubsection{Reward}
%       white boxes : -1 \newline
%       green boxes : +10 (only the first time)\newline
%       red boxes : -10 \newline
%       
%       \subsubsection{State inconsistency}
%       
%       Imagine your in the boxe (6,0), you only know that you are in this box, you don't already know 
%       if you reach the goal in (4,0). Thus, there is two different reasonable chooses.

      \newpage
      \section{Effects of advices}
      
      In this section, we are experimenting how the advice should affect the learner in reinforcement
      learning algorithms like Q-Learning or Sarsa.
      We are working with an approximation of the Q-function with a common linear function 
      \begin{footnotesize}
       $Q(s,a) = \sum\limits_{i} \theta_{i} \times f_{i}(s,a)$.
      \end{footnotesize}
      The features $f$ are binary and follow a tile coding, \cite{ReinforceLearningIntro}.
      Then, it is necessary to perform a gradient descent to approximate $\vec\theta$.
      Our advise is always the best action.
      
      
      \paragraph{Notations}
      
%       Let us also introduce some notations :
\begin{itemize}
 \item Here $a$ is the recommended action, and $s$ the current state.
 \item $\vec\theta(f(s,a))$ represents the actived $\theta_i$ in 
      the state $s$ with the action $a$. It only make sense because the features $f$ are binary.
      Thus, $Q(s,a)$ can be rewrite like $Q(s,a) = \sum\limits_{i} \theta_{i}(f(s,a))$.
\end{itemize}

     
     Moreover, we asume that :
      \begin{center}
	$\forall s \in State, \forall a \in Actions(s), \forall a' \in Actions(s), |\vec\theta (f(s,a))| = | \vec\theta (f(s,a'))|$ 
      \end{center}
      with means that the number of actived features is always the same for a state, in case of using
      tile coding, it is trivial.
      

      
      \subsection{Strategies}
      
      \paragraph{Max} In this strategy, we increase the Q value of the recommended action, 
      with the best possible Q value in the current state, so that it is selected next time. 
      \newline
      Tabular version :
      \begin{center}
       $Q(s,a) = \underset{a'}{\operatorname{max}}\ Q(s,a') + \delta$
      \end{center}
      Approximation version :
      \begin{center}
	$\vec\theta(f(s,a)) = \vec\theta(f(s,a'))$
	
	$with\ a' = \underset{b}{\operatorname{argmax}}\ Q(s,b) $ 
	\end{center}
       
       \paragraph{Decrease} Instead of increasing the Q value of the recommanded action, we will decrease
       the Q-value of the others actions.
       
       
       \paragraph{Informed Exploration}
       This strategy can only be used in the case of advising before acting. Agent takes action $a$,
       and $Q(s,a)$ will be updating according with the next reward.
       
       
       
       \paragraph{Fixed} 
	$\pi(s)=a$ \newline
	$Q(s,a) = max(Q(s,\_) + \delta$

      \subsection{Advice after acting}
%       In this experiment, we have two agent, a teacher (QLearning) and a learner (QLearning).
%       The teacher is favored to advice (it is done to compare advice effects) : +10 if advice
%       else -10. With this reward function, it is not important to display the algorithm ``advice before''
%       because it will advice all the time.
%       
%       The teacher advices the learner with the best action to take (according to 
%       the best policy previously computed). They have the same state representation.
%             
%       \paragraph{Mountain Car}
%       \begin{figure}[H]
%       \begin{center}
% 	\includegraphics[width=320px]{graphA_MA}
% 	\caption{ Teacher advices all the time. Number of step to reach to goal at the first episod }
% 	\end{center}
%       \end{figure}
%       
%       The two best strategies are Override and Max+Override. (Max+Override is a little quite better because it
%       updates the Q value directly).
% %       Moreover, the Max strat is really better but 
%       
%       \paragraph{Grid World}
%       \begin{figure}[H]
%       \begin{center}
% 	\includegraphics[width=320px]{graphA_GA}
% 	\caption{ Teacher advices all the time. Number of step to reach to goal at the first episod }
% 	\end{center}
%       \end{figure}
%       
%       This time in the grid world, the best strategy is Max.
%       The fixed policy strat are really bad, more than without advice.
%       
%       \subsubsection{Grid World (LS)}
%       \begin{figure}[H]
%       \begin{center}
% 	\includegraphics[width=320px]{graphA_LA}
% 	\caption{ Teacher advices all the time. Number of step to reach to goal at the first episod }
% 	\end{center}
%       \end{figure}
%       
%       In this environnement, all strategies are bad when the teacher advice after because of 
%       the different good choose possible in a same state.
%       
      
      \subsection{Advice before acting}
      
      This experiment is used to compare the advice effect when the teacher can correct a mistake of the 
      learner before he perform it.
      We still give advice 10\% of time to compare the different strategy.
      
      
      \subsection{Conclusion}
      
      \section{Teacher Model}
      
      How the teacher is computed?
      
      \subsection{Heuristics}
      
      \cite{Torrey:2013:TBA:2484920.2485086} introduce different teacher model, where 2 reinforcement learning agent
      learn the same task. One have fully learn, the teacher, and the other one, the learner, starts.
      
      During the learning, using a heuristics, the learner will receive the best action compute by the teacher
      for a state. The number of advice is fixed.
      
      \subsection{Learn to teach}
      
      Now, we could like to introduce an other configuration, that we trust better.
      The teacher will not learn the same task that the learner. However, he computes the best policy internally. 
      He will have to solve an other problem, his state is define like 
      $Teacher\_States = Learner\_States \times Learner\_Actions$\\
      which is the state of the learner and the action he takes in this state.
      
      \paragraph{Reward functions}
      
      We have to choose which criterion we want to optimize :
      
      \subparagraph{Minimizing the number of step}

	  \begin{algorithmic}
	  \If {$giveAdvice$}
	      \State $r\gets -1 \times cost$
	  \Else
	      \If {$learner\_take\_best\_action$}
		  \State $r\gets 0$
	      \Else
		  \State $r\gets -1$
	      \EndIf
	  \EndIf
	  \end{algorithmic}

      
      \subparagraph{Maximazing the learner reward}
      
	  \begin{algorithmic}
	  \If {$giveAdvice$}
	      \State $r\gets -1  \times cost \times learner\_reward $
	  \Else
	      \State $r\gets learner\_reward$
	  \EndIf
	  \end{algorithmic}
	  
	  
	  We will mainly focus on minimizing the number of step.
      
      \subsubsection{Advice or not}
      
      Our first model will decide if the teacher have to advice with the best action or not.
      $Teacher\_Actions = \{\neg F, F\}$
      
      \subsubsection{Learn the action}
      
      In this section, we will change the model of the teacher, instead of having to choose to give a feedback or not,
      he can choose which action he wants to advice.
      
       $Teacher\_Actions = \{ \neg F\} \cup Learner\_Actions $
     
      
      \subsection{Comparaisons}
      
      
      \section{Conclusion}
      
      \subsection{Future research}
      
      Differents state representations.
      
      Softmax
      
      \subsection{Main results}
      
      
%       \begin{itemize}
%        \item Max
%        
%        \item Max + Override
%        \item Override (Lucky Exploration)
%       \end{itemize}



    %     \begin{center}
%       ${CM}^* = \underset{CM}{\operatorname{argmax}}\ p( S_1,...,S_n | CM ) $
%     \end{center}
%     \begin{center}
%       $p( S_1,...,S_n | CM ) = p(S_1|CM) \times \prod\limits_{i=2}^n p(S_i|S_{i-1}, CM)$
%     \end{center}
%     
%     Par ailleurs, nous allons nous concentrer sur les arrondissements du
%     $4^{i\grave{e}me}$ et du $16^{i\grave{e}me}$ arrondissement.
% 
%     \subsubsection{Données bruts}
% 
%       Moyennes réalisées sur une centaine de lancement.
%       
%       \begin{center}
% 	\begin{tabular}{cc}
% 	  \hspace*{-1cm}
% 	  \begin{minipage}[b]{.52\linewidth}
% 	    \begin{figure}[H]
% % 	      \includegraphics[width=270px]{e32call}
% 	      \caption{ Taux de classification  }
% 	    \end{figure}
% 	  \end{minipage}
% 	  &
% 	  \begin{minipage}[b]{.5\linewidth}
% 	    \begin{figure}[H]
% % 	      \includegraphics[width=270px]{e32calln}
% 	      \caption{ Taux avec normalisation  }
% 	    \end{figure}
% 	  \end{minipage}
% 	\end{tabular}
%       \end{center} 
%       
%       La première chose que l'on peut remarquer est l'interêt de la discrétisation des données puisque sans elle
%       on ne peut dépasser les 25\% de performance. Néanmoins, il ne faut pas créer de classe trop grande puisque
%       qu'au delà des intervales de taille 10, les performances vont diminuer petit à petit.
%       
%       La seconde information est celle de l'interêt de la normalisation, qui, comme on peut le voir ici, n'influe que
%       très peu sur les résultats.
%     
%     \subsubsection{Données différentielles}
%     
%     Nous allons maintenant utiliser un autre codage des données et modéliser le nombre de vélib arrivant ou partant
%     pour une station donnée. Ce sont également des moyennes réalisées sur une centaine de lancement.
%     
% 	\begin{center}
% 	\begin{tabular}{cc}
% 	  \hspace*{-1cm}
% 	  \begin{minipage}[b]{.52\linewidth}
% 	    \begin{figure}[H]
% % 	      \includegraphics[width=270px]{e32calls}
% 	      \caption{ Taux de classification  }
% 	    \end{figure}
% 	  \end{minipage}
% 	  &
% 	  \begin{minipage}[b]{.5\linewidth}
% 	    \begin{figure}[H]
% % 	      \includegraphics[width=270px]{e32callsn}
% 	      \caption{ Taux avec normalisation  }
% 	    \end{figure}
% 	  \end{minipage}
% 	\end{tabular}
%       \end{center} 
%     
%     
%     \subsection{Clustering}
%     
%     Nous allons maintenant utiliser les chaînes de Markov pour faire du clustering. Nous nous plaçons donc dans
%     le cadre de l'apprentissage non supervisé. Pour réaliser le clustering, nous utiliserons la famille d'algorithme
%     EM. Nous aurons une chaine de Markov par cluster que nous optimiserons.
%     
%     
%     \subsubsection{Données}
%     
%     Nous allons utiliser les mêmes données que précédent mais en prenant en compte tout les arrondissements de Paris.
%     
%     \begin{figure}[H]
%       \begin{center}
% % 	\includegraphics[width=270px]{paris}
% 	\caption{ Stations dans Paris }
% 	\end{center}
%     \end{figure}
%       
%     
%     \subsection{Clustering en pré-traitement}
%     
%     Il peut être interessant de prétraiter la classification par un clustering pour réduire la dimensionnalité des données
%     avant de les traiter.
%   
%   \section{Modèles de Markov Cachés}

      
      


      
%       \begin{figure}[H]
%       \begin{center}
% 	\includegraphics[width=270px]{donnes_gaussians}
% 	\caption{ Données gaussiennes linéairement séparables.\newline La frontrière est déterminer par adaline. }
% 	\end{center}
%       \end{figure}
%       
%       

%       	\begin{center}
% 	\begin{tabular}{cc}
% 	  \hspace*{-1cm}
% 	  \begin{minipage}[b]{.5\linewidth}
% 	    \begin{figure}[H]
% 	      \includegraphics[width=265px]{grad_convR}
% 	      \caption{ Evolution du critère d'erreur\\ en fonction des itérations. }
% 	    \end{figure}
% 	  \end{minipage}
% 	  &
% 	  \begin{minipage}[b]{.5\linewidth}
% 	    \begin{figure}[H]
% 	      \includegraphics[width=270px]{espaceWR}
% 	      \caption{ Espace du vecteur W\\[0.5cm]}
% 	    \end{figure}
% 	  \end{minipage}
% 	\end{tabular}
%       \end{center} 
      
      
%       
%       \begin{center}
% 	\begin{tabular}{cc}
% 	  \hspace*{-1cm}
% 	  \begin{minipage}[b]{.5\linewidth}
% 	    \begin{figure}[H]
% 	      \includegraphics[width=270px]{grad_convL}
% 	      \caption{ Evolution du critère d'erreur\\ en fonction des itérations. }
% 	    \end{figure}
% 	  \end{minipage}
% 	  &
% 	  \begin{minipage}[b]{.5\linewidth}
% 	    \begin{figure}[H]
% 	      \includegraphics[width=270px]{espaceWL}
% 	      \caption{ Espace du vecteur W\\[0.5cm]}
% 	    \end{figure}
% 	  \end{minipage}
% 	\end{tabular}
%       \end{center} 
      

%       \begin{center}
% 	\begin{tabular}{cc}
% 	  \hspace*{-1cm}
% 	  \begin{minipage}[b]{.5\linewidth}
% 	    \begin{figure}[H]
% 	      \includegraphics[width=270px]{grad_convD}
% 	      \caption{ Evolution du critère d'erreur\\ en fonction des itérations. }
% 	    \end{figure}
% 	  \end{minipage}
% 	  &
% 	  \begin{minipage}[b]{.5\linewidth}
% 	    \begin{figure}[H]
% 	      \includegraphics[width=270px]{espaceWD}
% 	      \caption{ Espace du vecteur W\\[0.5cm]}
% 	    \end{figure}
% 	  \end{minipage}
% 	\end{tabular}
%       \end{center} 
      

  \clearpage
  
  \printbibliography
  
%   \clearpage
%   \section{Performances}
%     \subsection{Rejet - knn}
%       \subsubsection{En ambiguité}
%       \subsubsection{En distance}
%     \subsection{Surapprentissage - adaline}
%     \subsection{Surapprentissage - knn}
%     Le surapprentissage de knn peut être visible sur la figure 10.
%     \subsection{Validation croisée - perceptron}
%     \subsection{Validation croisée - knn}

\end{document}



